import requests
import json
import os
import time
import glob
import base64
from datetime import datetime, timedelta
from tqdm import tqdm
from typing import List, Dict, Optional, Tuple

"""
GitHubçƒ­é—¨é¡¹ç›®åˆ†ææœºå™¨äºº
åŠŸèƒ½ï¼š
1. è·å–GitHubè¿‘æœŸçƒ­é—¨é¡¹ç›®æ•°æ®
2. ä½¿ç”¨AIåˆ†æé¡¹ç›®æŠ€æœ¯äº®ç‚¹
3. ç”ŸæˆMarkdownæ ¼å¼çš„æ´å¯ŸæŠ¥å‘Š
4. å¯¹æ¯”å†å²è¶‹åŠ¿å˜åŒ–
"""

def get_date_n_days_ago(days: int) -> str:
    """
    è·å–nå¤©å‰çš„ISOæ ¼å¼æ—¥æœŸå­—ç¬¦ä¸²
    
    Args:
        days: å¤©æ•°
        
    Returns:
        ISOæ ¼å¼æ—¥æœŸå­—ç¬¦ä¸² (e.g., "2025-07-24T15:48:57Z")
    """
    dt = datetime.now() - timedelta(days=days)
    return dt.strftime("%Y-%m-%dT%H:%M:%SZ")

def build_trending_url(days: int = 7, limit: int = 10) -> str:
    """
    æ„å»ºGitHubçƒ­é—¨é¡¹ç›®æŸ¥è¯¢URL
    
    Args:
        days: æŸ¥è¯¢å¤©æ•°èŒƒå›´
        limit: è¿”å›é¡¹ç›®æ•°é‡
        
    Returns:
        GitHub APIæŸ¥è¯¢URL
    """
    base_url = "https://api.github.com/search/repositories"
    since_date = get_date_n_days_ago(days)
    created_query = f"created:>{since_date}"
    return f"{base_url}?q={created_query}&sort=stars&order=desc&per_page={limit}"

def get_trending_repos(days: int = 7, limit: int = 10, token: Optional[str] = None) -> List[Dict]:
    """
    ä»GitHub APIè·å–çƒ­é—¨é¡¹ç›®åˆ—è¡¨
    
    Args:
        days: æŸ¥è¯¢å¤©æ•°èŒƒå›´
        limit: è¿”å›é¡¹ç›®æ•°é‡
        token: GitHub APIä»¤ç‰Œ
        
    Returns:
        çƒ­é—¨é¡¹ç›®åˆ—è¡¨ï¼Œæ¯ä¸ªé¡¹ç›®ä¸ºå­—å…¸æ ¼å¼
    """
    url = build_trending_url(days, limit)
    headers = {"Accept": "application/vnd.github.v3+json"}
    if token:
        headers["Authorization"] = f"token {token}"
        
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        return response.json().get('items', [])
    except requests.exceptions.RequestException as e:
        print(f"âŒ è·å–trendingä»“åº“å¤±è´¥: {e}")
        return []

from collections import Counter

def analyze_data(repos: List[Dict]) -> Optional[Dict]:
    """
    åˆ†æä»“åº“æ•°æ®ï¼Œç”Ÿæˆç»Ÿè®¡æŒ‡æ ‡
    
    Args:
        repos: ä»“åº“æ•°æ®åˆ—è¡¨
        
    Returns:
        åŒ…å«ç»Ÿè®¡æŒ‡æ ‡çš„å­—å…¸:
        {
            "repo_count": é¡¹ç›®æ•°é‡,
            "total_stars": æ€»æ˜Ÿæ ‡æ•°,
            "total_forks": æ€»Forkæ•°,
            "language_stats": è¯­è¨€åˆ†å¸ƒç»Ÿè®¡
        }
    """
    if not repos:
        return None

    # ç»Ÿè®¡è¯­è¨€åˆ†å¸ƒï¼ˆå¤„ç†Noneå€¼ï¼‰
    language_list = [repo['language'] or 'N/A' for repo in repos]
    language_stats = Counter(language_list)
    
    # è®¡ç®—æ˜Ÿæ ‡å’ŒForkæ€»æ•°
    total_stars = sum(repo['stargazers_count'] for repo in repos)
    total_forks = sum(repo.get('forks_count', 0) for repo in repos)
    
    return {
        "repo_count": len(repos),
        "total_stars": total_stars,
        "total_forks": total_forks,
        "language_stats": dict(language_stats.most_common())
    }

def print_console_report(repos: List[Dict], analysis_result: Dict):
    """
    åœ¨æ§åˆ¶å°æ‰“å°åˆ†ææŠ¥å‘Š
    
    Args:
        repos: ä»“åº“æ•°æ®åˆ—è¡¨
        analysis_result: åˆ†æç»“æœå­—å…¸
    """
    # æŠ¥å‘Šå¤´éƒ¨
    header = "ğŸ”¥ GitHub çƒ­é—¨é¡¹ç›®æ´å¯ŸæŠ¥å‘Š"
    separator = "=" * len(header)
    print(f"\n{separator}\n{header}\n{separator}")
    print(f"å…±æ‰¾åˆ° {analysis_result['repo_count']} ä¸ªçƒ­é—¨é¡¹ç›®")
    
    if not repos:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°çƒ­é—¨é¡¹ç›®")
        return
    
    # æ‰“å°æ¯ä¸ªé¡¹ç›®è¯¦æƒ…
    for i, repo in enumerate(repos, 1):
        print(f"\nğŸ† #{i} {repo['name']}")
        print(f"ğŸ”— é“¾æ¥: {repo['html_url']}")
        
        # å¤„ç†æè¿°æ–‡æœ¬
        description = repo.get('description') or 'æ— æè¿°'
        if len(description) > 100:
            description = description[:100] + "..."
        print(f"ğŸ“ æè¿°: {description}")
        
        print(f"â­ æ˜Ÿæ ‡æ•°: {repo['stargazers_count']}")
        
        # å¤„ç†è¯­è¨€æ˜¾ç¤º
        language = repo['language'] or 'æœªè¯†åˆ«'
        print(f"ğŸ’» ç¼–ç¨‹è¯­è¨€: {language}")
        
        # æ ¼å¼åŒ–æ—¥æœŸ
        for date_type in ['created_at', 'updated_at']:
            date_str = repo.get(date_type)
            if date_str:
                try:
                    dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                    label = "åˆ›å»ºæ—¶é—´" if date_type == 'created_at' else "æ›´æ–°æ—¶é—´"
                    print(f"ğŸ“… {label}: {dt.strftime('%Yå¹´%mæœˆ%dæ—¥')}")
                except ValueError:
                    print(f"ğŸ“… {label}: {date_str}")
        
        print("-" * 50)
    
    # æ‰“å°è¯­è¨€åˆ†å¸ƒï¼ˆæŒ‰é¡¹ç›®æ•°é‡é™åºæ’åºï¼‰
    print("\nç¼–ç¨‹è¯­è¨€åˆ†å¸ƒ:")
    sorted_stats = sorted(analysis_result['language_stats'].items(), 
                         key=lambda x: x[1], reverse=True)
    for lang, count in sorted_stats:
        lang_display = lang if lang else "N/A"
        print(f"  {lang_display}: {count} ä¸ªé¡¹ç›®")
    
    # æ‰“å°æ€»ç»“
    print(f"\nâ­ æ€»æ˜Ÿæ ‡æ•°: {analysis_result['total_stars']}")
    print(f"ğŸ´ æ€»Forkæ•°: {analysis_result['total_forks']}")
    print(separator)

def save_markdown_report(repos: List[Dict], analysis_result: Dict, days: int = 7, filename: Optional[str] = None) -> str:
    """
    å°†åˆ†æç»“æœä¿å­˜ä¸ºMarkdownæ ¼å¼çš„æŠ¥å‘Š
    
    Args:
        repos: é¡¹ç›®æ•°æ®åˆ—è¡¨
        analysis_result: åˆ†æç»“æœå­—å…¸
        days: æŸ¥è¯¢å¤©æ•°
        filename: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        
    Returns:
        ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    """
    today_str = datetime.now().strftime('%Y-%m-%d')
    if not filename:
        filename = f"github_trending_{days}days_{today_str}.md"
    query_time = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    github_token = os.getenv("GITHUB_TOKEN")
    ai_api_key = os.getenv("DEEPSEEK_API_KEY")
    
    if not github_token:
        print("âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°GITHUB_TOKENç¯å¢ƒå˜é‡ï¼Œå¯èƒ½å½±å“READMEè·å–")
    
    # æ·»åŠ AIåˆ†ææ‘˜è¦
    repos = enrich_repos_with_ai_summaries(repos, github_token, ai_api_key)
    
    # æ„å»ºé¡¹ç›®è¡¨æ ¼
    table_header = "| æ’å | ğŸ“¦ é¡¹ç›®åç§° | ğŸ‘¤ ä½œè€… | â­ Star | ğŸ´ Fork | è¯­è¨€ | ğŸ“ æè¿° |\n"
    table_separator = "|------|----------|------|--------|--------|------|------|\n"
    
    table_rows = []
    for i, repo in enumerate(repos, 1):
        # é¡¹ç›®åŸºæœ¬ä¿¡æ¯
        name_link = f"[{repo['name']}]({repo['html_url']})"
        owner = repo.get('owner', {})
        author_name = owner.get('login', 'N/A')
        author_link = owner.get('html_url', '')
        author_md = f"[{author_name}]({author_link})" if author_link else author_name
        stars = f"â­ {repo['stargazers_count']}"
        forks = f"ğŸ´ {repo.get('forks_count', 0)}"
        language = repo['language'] or 'N/A'
        
        # å¤„ç†æè¿°æ–‡æœ¬
        description = repo.get('description') or 'æ— æè¿°'
        description = description.replace('\n', ' ').replace('|', 'ï½œ')
        if len(description) > 50:
            description = description[:50] + "..."
        
        # æ·»åŠ AIåˆ†ææ‘˜è¦
        formatted_ai_summary = repo['ai_summary'].replace('\n', '\n> ')
        ai_summary_md = f"<br>> ğŸ¤– **AIå°ç»“**:\n> {formatted_ai_summary}"
        
        # åˆå¹¶æè¿°å’ŒAIåˆ†æ
        description_with_ai = f"{description}{ai_summary_md}"
        
        # æ·»åŠ è¡¨æ ¼è¡Œ
        table_rows.append(f"| {i} | {name_link} | {author_md} | {stars} | {forks} | {language} | {description_with_ai} |")
    
    # æ„å»ºè¯­è¨€åˆ†å¸ƒè¡¨æ ¼ï¼ˆæŒ‰é¡¹ç›®æ•°é‡é™åºæ’åºï¼‰
    sorted_stats = sorted(analysis_result['language_stats'].items(), 
                         key=lambda x: x[1], reverse=True)
    lang_table = [
        "| è¯­è¨€ | é¡¹ç›®æ•°é‡ |",
        "| :--- | :------- |",  # å·¦å¯¹é½
        *[f"| {lang if lang else 'N/A'} | {count}ä¸ªé¡¹ç›® |" 
          for lang, count in sorted_stats]
    ]
    
    # å†™å…¥Markdownæ–‡ä»¶
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            # æŠ¥å‘Šæ ‡é¢˜
            f.write("# ğŸ”¥ GitHub çƒ­é—¨é¡¹ç›®æ´å¯ŸæŠ¥å‘Š\n\n")
            f.write(f"**æŸ¥è¯¢æ—¶é—´ï¼š** {query_time}  ")
            f.write(f"**æ—¶é—´èŒƒå›´ï¼š** æœ€è¿‘{days}å¤©  ")
            f.write(f"**é¡¹ç›®æ•°é‡ï¼š** {len(repos)}ä¸ª\n\n")
            
            # é¡¹ç›®æ’è¡Œæ¦œ
            f.write("## ğŸ† çƒ­é—¨é¡¹ç›®æ’è¡Œæ¦œ\n\n")
            f.write(table_header)
            f.write(table_separator)
            f.write("\n".join(table_rows))
            f.write("\n\n")
            
            # è¯­è¨€åˆ†å¸ƒ
            f.write("## ğŸ“Š ç¼–ç¨‹è¯­è¨€åˆ†å¸ƒ\n\n")
            f.write("\n".join(lang_table))
            f.write("\n\n")
            
            # æ€»ç»“
            f.write(f"â­ æ€»æ˜Ÿæ ‡æ•°: {analysis_result['total_stars']}  ")
            f.write(f"ğŸ´ æ€»Forkæ•°: {analysis_result['total_forks']}\n")
            f.write("\n> æ³¨ï¼šN/A è¡¨ç¤ºè¯¥é¡¹ç›®æœªæŒ‡å®šä¸»è¯­è¨€ï¼Œæˆ–ä¸ºæ–‡æ¡£/å¤šè¯­è¨€é¡¹ç›®ã€‚\n")
        
        print(f"âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜: {filename}")
        return filename
    except Exception as e:
        print(f"âŒ ä¿å­˜MarkdownæŠ¥å‘Šå¤±è´¥: {e}")
        return ""

# åˆ›å»ºREADMEç¼“å­˜ç›®å½•
README_CACHE_DIR = "readme_cache"
if not os.path.exists(README_CACHE_DIR):
    os.makedirs(README_CACHE_DIR)

def enrich_repos_with_ai_summaries(repos: List[Dict], github_token: Optional[str], ai_api_key: Optional[str]) -> List[Dict]:
    """
    ä¸ºæ¯ä¸ªä»“åº“æ·»åŠ AIç”Ÿæˆçš„æ‘˜è¦
    
    Args:
        repos: ä»“åº“åˆ—è¡¨
        github_token: GitHub APIä»¤ç‰Œ
        ai_api_key: DeepSeek APIå¯†é’¥
        
    Returns:
        æ·»åŠ äº†ai_summaryå­—æ®µçš„ä»“åº“åˆ—è¡¨
    """
    # æ£€æŸ¥APIå¯†é’¥
    if not ai_api_key:
        print("âš ï¸ æœªæ£€æµ‹åˆ°AI API Keyï¼Œè·³è¿‡AIæ€»ç»“")
        for repo in repos:
            repo['ai_summary'] = "AIæ€»ç»“åŠŸèƒ½æœªé…ç½®ã€‚"
        return repos
    
    print(f"â³ å¼€å§‹ä¸º {len(repos)} ä¸ªé¡¹ç›®ç”ŸæˆAIåˆ†ææ‘˜è¦...")
    
    # ä½¿ç”¨è¿›åº¦æ¡å¤„ç†æ¯ä¸ªä»“åº“
    for repo in tqdm(repos, desc="ç”ŸæˆAIæ€»ç»“"):
        # è·å–READMEå†…å®¹
        readme_content = get_readme_content(repo['full_name'], github_token)
        
        if readme_content:
            # ç”ŸæˆAIæ‘˜è¦
            repo['ai_summary'] = summarize_with_ai(readme_content, ai_api_key, repo)
        else:
            repo['ai_summary'] = "æœªèƒ½è·å–READMEå†…å®¹ã€‚"
    
    return repos

def get_readme_content(repo_full_name: str, token: Optional[str]) -> Optional[str]:
    """
    è·å–ä»“åº“çš„READMEå†…å®¹ï¼ˆå¸¦ç¼“å­˜æœºåˆ¶ï¼‰
    
    Args:
        repo_full_name: ä»“åº“å…¨åï¼ˆowner/repoï¼‰
        token: GitHub APIä»¤ç‰Œ
        
    Returns:
        READMEå†…å®¹å­—ç¬¦ä¸²ï¼Œè·å–å¤±è´¥è¿”å›None
    """
    # åˆ›å»ºå®‰å…¨çš„ç¼“å­˜æ–‡ä»¶å
    cache_filename = f"{repo_full_name.replace('/', '__')}.md"
    cache_path = os.path.join(README_CACHE_DIR, cache_filename)
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆï¼ˆ7å¤©å†…ï¼‰
    if os.path.exists(cache_path):
        cache_age = time.time() - os.path.getmtime(cache_path)
        if cache_age < 60 * 60 * 24 * 7:  # 7å¤©æœ‰æ•ˆæœŸ
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return f.read()
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        else:
            print(f"â™»ï¸ ç¼“å­˜å·²è¿‡æœŸ: {cache_filename}")
    
    # æ²¡æœ‰æœ‰æ•ˆç¼“å­˜ï¼Œä»GitHub APIè·å–
    if not token:
        print(f"âš ï¸ ç¼ºå°‘GitHub tokenï¼Œæ— æ³•è·å– {repo_full_name} çš„README")
        return None
    
    url = f"https://api.github.com/repos/{repo_full_name}/readme"
    headers = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github.v3+json"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        content_base64 = response.json().get('content', '')
        decoded_content = base64.b64decode(content_base64).decode('utf-8')
        
        # ä¿å­˜åˆ°ç¼“å­˜
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                f.write(decoded_content)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
            
        return decoded_content
    except requests.exceptions.RequestException as e:
        print(f"âŒ è·å–READMEå¤±è´¥: {e}")
        return None

def summarize_with_ai(content: str, api_key: str, repo: Dict) -> str:
    """
    ä½¿ç”¨DeepSeek APIç”Ÿæˆé¡¹ç›®æ‘˜è¦
    
    Args:
        content: READMEå†…å®¹
        api_key: DeepSeek APIå¯†é’¥
        repo: ä»“åº“ä¿¡æ¯å­—å…¸
        
    Returns:
        AIç”Ÿæˆçš„æ‘˜è¦å­—ç¬¦ä¸²
    """
    # æ·»åŠ APIè¯·æ±‚é—´éš”é˜²æ­¢é€Ÿç‡é™åˆ¶
    time.sleep(1.2)
    
    # æ˜¾ç¤ºå¤„ç†è¿›åº¦
    content_size = len(content) / 1024  # KB
    print(f"  - åˆ†æ: {repo['name']} ({content_size:.1f}KB)")
    
    # æˆªæ–­è¿‡é•¿çš„å†…å®¹
    if len(content) > 12000:
        content = content[:12000] + "...[å†…å®¹æˆªæ–­]"
    
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # æ„å»ºåˆ†ææç¤º
        prompt = f"""
ä½œä¸ºGitHubè¶‹åŠ¿åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹é¡¹ç›®ï¼š

é¡¹ç›®ä¿¡æ¯ï¼š
- åç§°ï¼š{repo['name']}
- ä¸»è¦è¯­è¨€ï¼š{repo['language'] or 'æœªæŒ‡å®š'}
- æ˜Ÿæ ‡æ•°ï¼š{repo['stargazers_count']}
- æè¿°ï¼š{repo.get('description', 'æ— ')}

READMEå†…å®¹ï¼š
{content}

è¯·ä»ä»¥ä¸‹3ä¸ªæ–¹é¢åˆ†æï¼š
1. ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ä¸è§£å†³çš„é—®é¢˜
2. ğŸ’¡ æŠ€æœ¯äº®ç‚¹ä¸åˆ›æ–°ç‚¹
3. ğŸ”¥ è¿‘æœŸå…³æ³¨åº¦é«˜çš„åŸå› 

è¦æ±‚ï¼š
- æ¯ç‚¹ç”¨"- "å¼€å¤´
- æ¯ç‚¹ä¸è¶…è¿‡40å­—
- ä½¿ç”¨ä¸“ä¸šä½†ç®€æ´çš„æŠ€æœ¯è¯­è¨€
"""
        
        payload = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 500,
            "temperature": 0.3  # é™ä½éšæœºæ€§
        }
        
        # å‘é€APIè¯·æ±‚
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions", 
            headers=headers, 
            json=payload,
            timeout=30
        )
        response.raise_for_status()
        
        # æå–AIå“åº”
        result = response.json()
        return result['choices'][0]['message']['content']
    except requests.exceptions.RequestException as e:
        print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
        return "AIåˆ†æè¯·æ±‚å¤±è´¥"
    except (KeyError, ValueError) as e:
        print(f"âŒ è§£æAPIå“åº”å¤±è´¥: {e}")
        return "è§£æAIå“åº”å¤±è´¥"

def save_raw_data(repos: List[Dict], filename: str = "trending_repos.json"):
    """
    ä¿å­˜åŸå§‹é¡¹ç›®æ•°æ®åˆ°JSONæ–‡ä»¶
    
    Args:
        repos: é¡¹ç›®æ•°æ®åˆ—è¡¨
        filename: è¾“å‡ºæ–‡ä»¶å
    """
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(repos, f, ensure_ascii=False, indent=2)
        print(f"âœ… åŸå§‹æ•°æ®å·²ä¿å­˜: {filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜åŸå§‹æ•°æ®å¤±è´¥: {e}")

def compare_trends(previous_analysis: Dict, current_analysis: Dict, 
                  previous_repos: List[Dict], current_repos: List[Dict]):
    """
    å¯¹æ¯”ä¸¤æ¬¡è¶‹åŠ¿åˆ†æç»“æœï¼Œç”Ÿæˆå˜åŒ–æŠ¥å‘Š
    
    Args:
        previous_analysis: å†å²åˆ†æç»“æœ
        current_analysis: å½“å‰åˆ†æç»“æœ
        previous_repos: å†å²é¡¹ç›®åˆ—è¡¨
        current_repos: å½“å‰é¡¹ç›®åˆ—è¡¨
    """
    # æŠ¥å‘Šå¤´éƒ¨
    header = "ğŸ“ˆ GitHubè¶‹åŠ¿å˜åŒ–åˆ†ææŠ¥å‘Š"
    separator = "=" * len(header)
    print(f"\n{separator}\n{header}\n{separator}")

    # å®è§‚æŒ‡æ ‡å¯¹æ¯”
    def format_change(prev, curr, label):
        change = curr - prev
        pct_change = (change / prev * 100) if prev != 0 else float('inf')
        pct_str = f"{pct_change:+.2f}%" if prev != 0 else "N/A"
        print(f"{label}: {prev} â†’ {curr} (å˜åŒ–: {change:+d}, {pct_str})")
    
    format_change(previous_analysis['total_stars'], current_analysis['total_stars'], "â­ æ€»æ˜Ÿæ ‡æ•°")
    format_change(previous_analysis['total_forks'], current_analysis['total_forks'], "ğŸ´ æ€»Forkæ•°")
    
    # è¯­è¨€çƒ­åº¦å˜åŒ–
    print("\nğŸ’» ç¼–ç¨‹è¯­è¨€çƒ­åº¦å˜åŒ–:")
    all_langs = set(previous_analysis['language_stats'].keys()) | set(current_analysis['language_stats'].keys())
    
    for lang in sorted(all_langs):
        prev_count = previous_analysis['language_stats'].get(lang, 0)
        curr_count = current_analysis['language_stats'].get(lang, 0)
        if prev_count != curr_count:
            change = curr_count - prev_count
            pct_change = (change / prev_count * 100) if prev_count != 0 else float('inf')
            pct_str = f"{pct_change:+.2f}%" if prev_count != 0 else "N/A"
            print(f"  - {lang}: {prev_count} â†’ {curr_count} (å˜åŒ–: {change:+d}, {pct_str})")

    # 3. é¡¹ç›®æ’åå˜åŒ–
    # åˆ›å»ºå†å²é¡¹ç›®å­—å…¸ï¼ˆåŒ…å«æ’åå’Œæ˜Ÿæ ‡æ•°ï¼‰
    prev_dict = {}
    for i, repo in enumerate(previous_repos, 1):
        prev_dict[repo['full_name']] = {
            'rank': i,
            'stars': repo['stargazers_count']
        }
    
    curr_dict = {}
    for i, repo in enumerate(current_repos, 1):
        curr_dict[repo['full_name']] = {
            'rank': i,
            'stars': repo['stargazers_count']
        }
    
    # æ–°ä¸Šæ¦œé¡¹ç›®
    new_projects = [repo for repo in current_repos if repo['full_name'] not in prev_dict]
    if new_projects:
        print(f"\nğŸ†• æ–°ä¸Šæ¦œé¡¹ç›® ({len(new_projects)}ä¸ª):")
        for repo in new_projects:
            print(f"  #{curr_dict[repo['full_name']]['rank']} {repo['name']} â­{repo['stargazers_count']}")
    
    # æ’åå˜åŒ–é¡¹ç›®
    rank_changes = []
    for repo in current_repos:
        name = repo['full_name']
        if name in prev_dict:
            prev_rank = prev_dict[name]['rank']
            curr_rank = curr_dict[name]['rank']
            if prev_rank != curr_rank:
                change = prev_rank - curr_rank
                # æ·»åŠ å†å²æ˜Ÿæ ‡æ•°ç”¨äºè®¡ç®—å˜åŒ–ç‡
                repo['prev_stars'] = prev_dict[name]['stars']
                rank_changes.append((repo, prev_rank, curr_rank, change))
    
    if rank_changes:
        print(f"\nğŸ“Š æ’åå˜åŒ– ({len(rank_changes)}ä¸ª):")
        # æŒ‰å˜åŒ–å¹…åº¦ä»å¤§åˆ°å°æ’åº
        for repo, prev_rank, curr_rank, change in sorted(rank_changes, key=lambda x: abs(x[3]), reverse=True):
            direction = "ğŸ“ˆ" if change > 0 else "ğŸ“‰"  # æ­£å˜åŒ–è¡¨ç¤ºæ’åä¸Šå‡
            # è®¡ç®—æ˜Ÿæ ‡æ•°å˜åŒ–ç‡ï¼ˆç›¸å¯¹äºå†å²æ˜Ÿæ ‡æ•°ï¼‰
            star_change = repo['stargazers_count'] - repo['prev_stars']
            star_pct = (star_change / repo['prev_stars'] * 100) if repo['prev_stars'] != 0 else float('inf')
            star_pct_str = f"{star_pct:+.2f}%" if repo['prev_stars'] != 0 else "N/A"
            print(f"  {direction} {repo['name']}: #{prev_rank} â†’ #{curr_rank} (æ’åå˜åŒ–: {change:+d}, æ˜Ÿæ ‡å˜åŒ–ç‡: {star_pct_str})")

    print("=" * 50)

def find_comparison_file(days_ago: int, data_dir: str = 'data') -> Optional[str]:
    """
    æŸ¥æ‰¾ä¸æŒ‡å®šå¤©æ•°æœ€åŒ¹é…çš„å†å²æ•°æ®æ–‡ä»¶
    
    Args:
        days_ago: è¦å¯¹æ¯”çš„å¤©æ•°ï¼ˆå¦‚7è¡¨ç¤º7å¤©å‰ï¼‰
        data_dir: æ•°æ®æ–‡ä»¶ç›®å½•
        
    Returns:
        æœ€åŒ¹é…çš„æ–‡ä»¶è·¯å¾„ï¼Œæ‰¾ä¸åˆ°è¿”å›None
    """
    target_date = datetime.now() - timedelta(days=days_ago)
    pattern = os.path.join(data_dir, "data_*.json")
    files = glob.glob(pattern)
    
    if not files:
        return None

    # æŸ¥æ‰¾æ—¥æœŸæœ€æ¥è¿‘çš„æ–‡ä»¶
    closest_file = None
    min_diff = float('inf')
    
    for file_path in files:
        try:
            # ä»æ–‡ä»¶åæå–æ—¥æœŸ
            filename = os.path.basename(file_path)
            date_str = filename[5:-5]  # æå–"data_YYYY-MM-DD.json"ä¸­çš„æ—¥æœŸéƒ¨åˆ†
            file_date = datetime.strptime(date_str, '%Y-%m-%d')
            
            # è®¡ç®—æ—¥æœŸå·®
            date_diff = abs((target_date - file_date).days)
            
            # æ›´æ–°æœ€æ¥è¿‘çš„æ–‡ä»¶
            if date_diff < min_diff:
                min_diff = date_diff
                closest_file = file_path
        except ValueError:
            continue  # è·³è¿‡æ ¼å¼ä¸æ­£ç¡®çš„æ–‡ä»¶å
    
    # åªè¿”å›æ—¥æœŸå·®åœ¨2å¤©å†…çš„æ–‡ä»¶
    return closest_file if min_diff <= 2 else None

def main():
    """ä¸»å‡½æ•°ï¼Œåè°ƒæ•´ä¸ªåˆ†ææµç¨‹"""
    # æ£€æŸ¥GitHubä»¤ç‰Œ
    token = os.getenv("GITHUB_TOKEN")
    if not token:
        print("âŒ æœªæ£€æµ‹åˆ°GITHUB_TOKENç¯å¢ƒå˜é‡ï¼Œè¯·æ£€æŸ¥è®¾ç½®ï¼")
        return
    
    # æ£€æŸ¥DeepSeek APIå¯†é’¥
    if not os.getenv("DEEPSEEK_API_KEY"):
        print("âš ï¸ æ³¨æ„ï¼šæœªæ£€æµ‹åˆ°DEEPSEEK_API_KEYç¯å¢ƒå˜é‡ï¼ŒAIæ€»ç»“åŠŸèƒ½å°†ä¸å¯ç”¨")
        print("   è®¾ç½®æ–¹æ³•:")
        print('   Windows: $env:DEEPSEEK_API_KEY="your_api_key"')
        print('   Linux/macOS: export DEEPSEEK_API_KEY="your_api_key"')
    
    # ç¨‹åºæ ‡é¢˜
    title = "ğŸ”¥ GitHubçƒ­é—¨é¡¹ç›®æ´å¯Ÿæœºå™¨äºº"
    print(f"\n{title}\n{'=' * len(title)}")
    
    # è·å–ç”¨æˆ·è¾“å…¥
    try:
        days_input = input("è¯·è¾“å…¥æŸ¥è¯¢å¤©æ•° (é»˜è®¤7å¤©): ").strip()
        days = int(days_input) if days_input.isdigit() else 7
        
        limit_input = input("è¯·è¾“å…¥é¡¹ç›®æ•°é‡ (é»˜è®¤10ä¸ª): ").strip()
        limit = int(limit_input) if limit_input.isdigit() else 10
    except ValueError:
        print("âš ï¸ è¾“å…¥æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼")
        days, limit = 7, 10
    
    print(f"\nğŸ”„ è·å–æœ€è¿‘{days}å¤©çš„Top {limit}çƒ­é—¨é¡¹ç›®...")
    
    # è·å–å¹¶åˆ†ææ•°æ®
    repos = get_trending_repos(days=days, limit=limit, token=token)
    if not repos:
        print("âš ï¸ æœªè·å–åˆ°çƒ­é—¨é¡¹ç›®ï¼Œç¨‹åºç»ˆæ­¢")
        return
    
    analysis_result = analyze_data(repos)
    
    # å‡†å¤‡ç›®å½•ç»“æ„
    data_dir = 'data'
    reports_dir = 'reports'
    os.makedirs(data_dir, exist_ok=True)
    os.makedirs(reports_dir, exist_ok=True)
    
    # ä¿å­˜åŸå§‹æ•°æ®
    today_str = datetime.now().strftime('%Y-%m-%d')
    raw_data_path = os.path.join(data_dir, f"data_{today_str}.json")
    save_raw_data(repos, filename=raw_data_path)
    
    # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
    print_console_report(repos, analysis_result)
    report_filename = os.path.join(reports_dir, f"github_trending_{days}days_{today_str}.md")
    save_markdown_report(repos, analysis_result, days=days, filename=report_filename)
    
    # è¶‹åŠ¿å¯¹æ¯”åˆ†æ
    print("\nğŸ“Š æ­£åœ¨åˆ†æè¶‹åŠ¿å˜åŒ–...")
    try:
        comparison_file = find_comparison_file(days, data_dir)
        if comparison_file:
            print(f"ğŸ” æ‰¾åˆ°å¯¹æ¯”æ–‡ä»¶: {comparison_file}")
            with open(comparison_file, 'r', encoding='utf-8') as f:
                previous_data = json.load(f)
            
            previous_analysis = analyze_data(previous_data)
            compare_trends(previous_analysis, analysis_result, previous_data, repos)
        else:
            print("â„¹ï¸ æœªæ‰¾åˆ°åˆé€‚çš„å†å²æ•°æ®è¿›è¡Œå¯¹æ¯”")
    except Exception as e:
        print(f"âŒ è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
